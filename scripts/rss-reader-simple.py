#!/usr/bin/env python3
"""
RSS é˜…è¯»å™¨ - æ— ä¾èµ–ç‰ˆæœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼‰
ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æï¼Œæ›´åŠ å®¹é”™
"""

import json
import sys
import re
from datetime import datetime, timezone
from pathlib import Path
import urllib.request
import urllib.error
import html

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = Path(__file__).parent.parent / "rss-feeds.json"
STATE_FILE = Path(__file__).parent.parent / "rss-state.json"


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if not CONFIG_FILE.exists():
        return None
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_state():
    """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
    if STATE_FILE.exists():
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"seen_entries": {}, "last_check": None}


def save_state(state):
    """ä¿å­˜çŠ¶æ€æ–‡ä»¶"""
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def fetch_rss(url, timeout=15):
    """è·å– RSS å†…å®¹"""
    try:
        req = urllib.request.Request(
            url,
            headers={'User-Agent': 'Mozilla/5.0 (Compatible; RSSReader/1.0)'}
        )
        with urllib.request.urlopen(req, timeout=timeout) as response:
            content = response.read()
            # å°è¯•ä¸åŒç¼–ç 
            for encoding in ['utf-8', 'gbk', 'gb2312', 'iso-8859-1']:
                try:
                    return content.decode(encoding)
                except:
                    continue
            return content.decode('utf-8', errors='ignore')
    except urllib.error.URLError as e:
        print(f"  âŒ ç½‘ç»œé”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"  âŒ è·å–å¤±è´¥: {e}")
        return None


def strip_html(text):
    """ç§»é™¤ HTML æ ‡ç­¾"""
    if not text:
        return ''
    # ç§»é™¤ script å’Œ style æ ‡ç­¾åŠå…¶å†…å®¹
    text = re.sub(r'<script[^>]*?>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r'<style[^>]*?>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)
    # ç§»é™¤æ‰€æœ‰ HTML æ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # è§£ç  HTML å®ä½“
    text = html.unescape(text)
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def parse_rss_regex(xml_content, feed_name):
    """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æ RSSï¼ˆæ›´å®¹é”™ï¼‰"""
    entries = []

    try:
        # æå–æ‰€æœ‰ <item> æˆ– <entry> å—
        items = re.findall(r'<item[^>]*>(.*?)</item>', xml_content, re.DOTALL | re.IGNORECASE)
        if not items:
            items = re.findall(r'<entry[^>]*>(.*?)</entry>', xml_content, re.DOTALL | re.IGNORECASE)

        for item in items[:15]:  # æœ€å¤šå–15æ¡
            entry = {}

            # æå–æ ‡é¢˜
            title_match = re.search(r'<title[^>]*>(.*?)</title>', item, re.DOTALL | re.IGNORECASE)
            if title_match:
                entry['title'] = strip_html(title_match.group(1))
            else:
                entry['title'] = 'æ— æ ‡é¢˜'

            # æå–é“¾æ¥
            link_match = re.search(r'<link[^>]*>(.*?)</link>', item, re.DOTALL | re.IGNORECASE)
            if not link_match:
                link_match = re.search(r'<link[^>]*url=["\']([^"\']+)["\']', item, re.IGNORECASE)
            if link_match:
                entry['link'] = strip_html(link_match.group(1) if link_match.lastindex else link_match.group(1))
            else:
                entry['link'] = ''

            # æå–æè¿°
            desc_match = re.search(r'<description[^>]*>(.*?)</description>', item, re.DOTALL | re.IGNORECASE)
            if not desc_match:
                desc_match = re.search(r'<content[^>]*>(.*?)</content>', item, re.DOTALL | re.IGNORECASE)
            if desc_match:
                desc = strip_html(desc_match.group(1))
                entry['summary'] = desc[:200] + '...' if len(desc) > 200 else desc
            else:
                entry['summary'] = ''

            # æå–å‘å¸ƒæ—¥æœŸ
            pub_match = re.search(r'<pubDate[^>]*>(.*?)</pubDate>', item, re.DOTALL | re.IGNORECASE)
            if not pub_match:
                pub_match = re.search(r'<published[^>]*>(.*?)</published>', item, re.DOTALL | re.IGNORECASE)
            if pub_match:
                entry['published'] = strip_html(pub_match.group(1))
            else:
                entry['published'] = ''

            # ä½¿ç”¨é“¾æ¥+æ ‡é¢˜ä½œä¸ºå”¯ä¸€ ID
            entry['id'] = entry['link'] + '|' + entry['title']

            entries.append(entry)

    except Exception as e:
        print(f"  âŒ è§£æé”™è¯¯: {e}")

    return entries


def format_entry(entry, feed_name):
    """æ ¼å¼åŒ–å•ç¯‡æ–‡ç« """
    title = entry.get('title', 'æ— æ ‡é¢˜')
    link = entry.get('link', '')
    published = entry.get('published', '')
    summary = entry.get('summary', '')

    result = f"### {title}\n\n"
    if summary:
        result += f"{summary}\n\n"
    if published:
        result += f"ğŸ“… {published}\n"
    result += f"ğŸ”— {link}\n"

    return result


def check_feeds(config, state):
    """æ£€æŸ¥æ‰€æœ‰è®¢é˜…çš„ RSS"""
    if not config:
        print("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º")
        return None

    all_new_entries = []
    seen_entries = state.get("seen_entries", {})

    for feed_config in config.get("feeds", []):
        if not feed_config.get("enabled", True):
            continue

        feed_name = feed_config.get("name", "æœªçŸ¥æ¥æº")
        feed_url = feed_config.get("url")
        category = feed_config.get("category", "æœªåˆ†ç±»")

        print(f"ğŸ“¡ {feed_name}")
        print(f"   {feed_url}")

        # è·å– RSS
        xml_content = fetch_rss(feed_url)
        if not xml_content:
            print(f"   â­ï¸  è·³è¿‡\n")
            continue

        # è§£æ
        entries = parse_rss_regex(xml_content, feed_name)
        if not entries:
            print(f"   â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°æ–‡ç« \n")
            continue

        max_items = config.get("settings", {}).get("maxItemsPerFeed", 5)
        new_count = 0

        for entry in entries[:max_items]:
            entry_id = entry.get('id')

            if entry_id and entry_id not in seen_entries:
                seen_entries[entry_id] = {
                    "title": entry.get('title', ''),
                    "seen_at": datetime.now(timezone.utc).isoformat()
                }

                formatted = format_entry(entry, feed_name)
                all_new_entries.append({
                    "feed": feed_name,
                    "category": category,
                    "content": formatted
                })
                new_count += 1

        print(f"   âœ… å‘ç° {new_count} ç¯‡æ–°æ–‡ç« \n")

    # æ›´æ–°çŠ¶æ€
    state["last_check"] = datetime.now(timezone.utc).isoformat()
    save_state(state)

    return all_new_entries


def generate_report(new_entries, config):
    """ç”ŸæˆæŠ¥å‘Š"""
    if not new_entries:
        return None

    # æŒ‰åˆ†ç±»åˆ†ç»„
    by_category = {}
    for entry in new_entries:
        cat = entry["category"]
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(entry)

    # ç”ŸæˆæŠ¥å‘Š
    from datetime import timedelta
    now_utc = datetime.now(timezone.utc)
    now_cn = now_utc + timedelta(hours=8)

    report_lines = [
        f"# ğŸ“° RSS é˜…è¯»æ‘˜è¦",
        f"",
        f"**æ—¶é—´**: {now_cn.strftime('%Y-%m-%d %H:%M')} (åŒ—äº¬æ—¶é—´)",
        f"**æ–°æ–‡ç« **: {len(new_entries)} ç¯‡",
        f""
    ]

    include_categories = config.get("settings", {}).get("includeCategories", ["å…¨éƒ¨"])

    for category in sorted(by_category.keys()):
        if "å…¨éƒ¨" not in include_categories and category not in include_categories:
            continue

        report_lines.append(f"\n---\n")
        report_lines.append(f"## ğŸ“‚ {category}\n")

        for entry in by_category[category]:
            report_lines.append(entry["content"])
            report_lines.append("")

    return "\n".join(report_lines)


def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ“¡ RSS é˜…è¯»å™¨")
    print("="*60 + "\n")

    # åŠ è½½é…ç½®
    config = load_config()
    if not config:
        print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º rss-feeds.json")
        sys.exit(1)

    # åŠ è½½çŠ¶æ€
    state = load_state()
    last_check = state.get("last_check")
    if last_check:
        print(f"ğŸ“… ä¸Šæ¬¡æ£€æŸ¥: {last_check}\n")

    # æ£€æŸ¥è®¢é˜…
    new_entries = check_feeds(config, state)

    # ç”ŸæˆæŠ¥å‘Š
    if new_entries:
        print(f"\n{'='*60}")
        print(f"âœ… æ€»å…±å‘ç° {len(new_entries)} ç¯‡æ–°æ–‡ç« ")
        print(f"{'='*60}\n")

        report = generate_report(new_entries, config)

        if report:
            # ä¿å­˜æŠ¥å‘Š
            report_file = Path(__file__).parent.parent / "rss-report.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)

            print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            print(f"\nğŸ’¡ å‘Šè¯‰æˆ‘ 'å‘é€ RSS æŠ¥å‘Š' å³å¯æ¨é€åˆ°é£ä¹¦\n")
    else:
        print("\nâœ… æ²¡æœ‰æ–°æ–‡ç« \n")


if __name__ == "__main__":
    main()
